{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import BigBull\n",
    "\n",
    "# 'conn_string' is a connection url for connecting to your databse\n",
    "# So, you have to install you own db before using this library.\n",
    "# more information plz, check http://docs.sqlalchemy.org/en/latest/core/engines.html\n",
    "# For example, if you use PostgreSQL for db\n",
    "# url = 'postgresql+psycopg2://username:password@localhost:5432/mydb'\n",
    "\n",
    "conn_string = \"mysql+pymysql://root:deepstock@localhost/deepstock?charset=utf8\"\n",
    "db = BigBull.StockDB(conn_string, BigBull.__meta)\n",
    "crawler = BigBull.StockCodeCrawler(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = BigBull.StockDB(conn_string, BigBull.__meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawler = BigBull.StockCodeCrawler(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist_crawler = BigBull.HistoryCrawler(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "종료\n",
      "DB에 데이터가 아무것도 없습니다.\n",
      "2008-01-01 00:00:00 ~ 2012-12-31 00:00:00 기간의 \n",
      "INSERT COUNT: 1248 데이터가 크롤 및 저장 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_crawler.save_history_of('삼성전자', '2008-1-1', '2012-12-31', hard_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-01-01 00:00:00 ~ 2012-12-31 00:00:00 기간의 \n",
      "데이터가 크롤 및 UPDATE 되었습니다. UPDATE COUNT: 1248 \n",
      "unique_req\n",
      "{datetime.datetime(2008, 1, 1, 0, 0), datetime.datetime(2012, 12, 30, 0, 0), datetime.datetime(2012, 12, 29, 0, 0), datetime.datetime(2012, 12, 31, 0, 0)}\n",
      "unique_db\n",
      "set()\n",
      "This is empty dict\n",
      "2008-01-01 00:00:00 ~ 2008-01-01 00:00:00 기간의 \n",
      "데이터가 정상적으로 크롤 및 INSERT 되지 않았습니다. INSERT COUNT: 0 \n",
      "This is empty dict\n",
      "2012-12-29 00:00:00 ~ 2012-12-31 00:00:00 기간의 \n",
      "데이터가 정상적으로 크롤 및 INSERT 되지 않았습니다. INSERT COUNT: 0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_crawler.save_history_of('삼성전자', '2008-1-1', '2012-12-31', hard_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_req\n",
      "{datetime.datetime(2012, 12, 30, 0, 0), datetime.datetime(2012, 12, 29, 0, 0), datetime.datetime(2012, 12, 31, 0, 0), datetime.datetime(2008, 1, 1, 0, 0)}\n",
      "unique_db\n",
      "set()\n",
      "종료\n",
      "This is empty dict\n",
      "2008-01-01 00:00:00 ~ 2008-01-01 00:00:00 기간의 \n",
      "데이터가 정상적으로 크롤 및 INSERT 되지 않았습니다. INSERT COUNT: 0 \n",
      "종료\n",
      "This is empty dict\n",
      "2012-12-29 00:00:00 ~ 2012-12-31 00:00:00 기간의 \n",
      "데이터가 정상적으로 크롤 및 INSERT 되지 않았습니다. INSERT COUNT: 0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_crawler.save_history_of('삼성전자', '2008-1-1', '2012-12-31', hard_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_req\n",
      "{datetime.datetime(2007, 12, 28, 0, 0), datetime.datetime(2007, 12, 21, 0, 0), datetime.datetime(2008, 1, 1, 0, 0), datetime.datetime(2013, 1, 4, 0, 0), datetime.datetime(2007, 12, 19, 0, 0), datetime.datetime(2007, 12, 18, 0, 0), datetime.datetime(2012, 12, 31, 0, 0), datetime.datetime(2007, 12, 26, 0, 0), datetime.datetime(2013, 1, 5, 0, 0), datetime.datetime(2007, 12, 22, 0, 0), datetime.datetime(2013, 1, 2, 0, 0), datetime.datetime(2007, 12, 30, 0, 0), datetime.datetime(2007, 12, 15, 0, 0), datetime.datetime(2007, 12, 17, 0, 0), datetime.datetime(2007, 12, 29, 0, 0), datetime.datetime(2007, 12, 23, 0, 0), datetime.datetime(2007, 12, 27, 0, 0), datetime.datetime(2007, 12, 24, 0, 0), datetime.datetime(2007, 12, 20, 0, 0), datetime.datetime(2013, 1, 7, 0, 0), datetime.datetime(2007, 12, 16, 0, 0), datetime.datetime(2013, 1, 6, 0, 0), datetime.datetime(2007, 12, 25, 0, 0), datetime.datetime(2013, 1, 1, 0, 0), datetime.datetime(2012, 12, 30, 0, 0), datetime.datetime(2013, 1, 3, 0, 0), datetime.datetime(2007, 12, 14, 0, 0), datetime.datetime(2007, 12, 31, 0, 0), datetime.datetime(2012, 12, 29, 0, 0), datetime.datetime(2007, 12, 13, 0, 0)}\n",
      "unique_db\n",
      "set()\n",
      "2007-12-13 00:00:00 ~ 2008-01-01 00:00:00 기간의 \n",
      "데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: 10 \n",
      "2012-12-29 00:00:00 ~ 2013-01-07 00:00:00 기간의 \n",
      "데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: 4 \n",
      "총 14개의 데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: 14 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_crawler.save_history_of('삼성전자', '2007-12-13', '2013-1-7', hard_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-01-08 00:00:00 ~ 2013-01-15 00:00:00 기간의 \n",
      "데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: 6 \n",
      "총 6개의 데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: 6 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_crawler.save_history_of('삼성전자', '2007-12-13', '2013-1-15', hard_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard_mode 설정이 False 이고\n",
      "요청하신 기간의 데이터는 이미 DB에 있으니\n",
      "아무런 작업이 일어나지 않습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_crawler.save_history_of('삼성전자', '2008-1-1', '2012-12-31', hard_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist_crawler.save_history_of('삼성전자', '2008-1-1', '2012-12-31', hard_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntergrityError:\n",
      "\t-error_code: 1062\n",
      "\t-error_msg: Duplicate entry '005930' for key 'PRIMARY'\n",
      "**** RollBack ****\n",
      "IntergrityError:\n",
      "\t-error_code: 1062\n",
      "\t-error_msg: Duplicate entry '091990' for key 'PRIMARY'\n",
      "**** RollBack ****\n"
     ]
    }
   ],
   "source": [
    "crawler.save_stock_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'005930'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.find_stock_code('삼성전자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSPI 회사는 DB에 없습니다.\n"
     ]
    }
   ],
   "source": [
    "db.find_stock_code('KOSPI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'017670'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.find_stock_code('SK텔레콤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'035720'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.find_stock_code('카카오')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'009150'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.find_stock_code('삼성전기')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2690"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1417+1273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2690"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in db.load_all_stock_code()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pprint\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sqlalchemy import select, desc, update\n",
    "from sqlalchemy.sql.expression import bindparam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YEAR_CHANGED = False # 병렬구조로 바꿀 때 local로 집어 넣어야함.\n",
    "\n",
    "# 가격정보 관련\n",
    "def filter_for_history(tag):\n",
    "    return (\n",
    "        tag.name == 'td'\n",
    "        and 'num' in tag.get('class', [])\n",
    "        and 'cDn' not in tag.get('class', [])\n",
    "        and 'cUp' not in tag.get('class', [])\n",
    "        and 'cFt' not in tag.get('class', [])\n",
    "    )\n",
    "\n",
    "def is_the_only_string_within_a_tag(s):\n",
    "    \"\"\"Return True if this string is the only child of its parent tag.\"\"\"\n",
    "    return s and (s == s.parent.string)\n",
    "\n",
    "def text_to_num(tag):\n",
    "    return int(tag.text.replace(\",\",\"\"))\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "# 시간관련\n",
    "def check_99_in_year(tags):\n",
    "    test = [tag.text.split('.')[0] for tag in tags]\n",
    "    if(test.count('99')):\n",
    "        return True, test\n",
    "    else:\n",
    "        return False, test\n",
    "\n",
    "def string_to_datetime(tags):\n",
    "    global YEAR_CHANGED\n",
    "    if(YEAR_CHANGED):\n",
    "        return [datetime.datetime.strptime('19'+tag.text, \"%Y.%m.%d\") for tag in tags]\n",
    "    else:\n",
    "        check_ninenine, test = check_99_in_year(tags)\n",
    "        if(check_ninenine):\n",
    "            print(\"--------99년 발견------------\")\n",
    "            YEAR_CHANGED = True\n",
    "            index = test.index('99')\n",
    "            return [datetime.datetime.strptime('20'+tag.text, \"%Y.%m.%d\") for tag in tags[:index]] + \\\n",
    "            [datetime.datetime.strptime('19'+tag.text, \"%Y.%m.%d\") for tag in tags[index:]]\n",
    "        else:\n",
    "            return [datetime.datetime.strptime('20'+tag.text, \"%Y.%m.%d\") for tag in tags]\n",
    "        \n",
    "def listdata_to_dict(ls):\n",
    "    temp_dict = {\n",
    "        'code': ls[0], # 종목코드\n",
    "        'date': ls[1], # 날짜\n",
    "        'open': ls[2], # 시가\n",
    "        'high': ls[3], # 고가\n",
    "        'low': ls[4], # 저가\n",
    "        'close': ls[5], # 종가\n",
    "        'volume': ls[6], #거래량\n",
    "    }\n",
    "    \n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YEAR_CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_finalpage(np_array, start_date):\n",
    "    if(len(np_array[np_array <= start_date])):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def base_crawl_method_for_history(company_name, start_date=datetime.datetime.min, end_date=datetime.datetime.today):\n",
    "    \"\"\"\n",
    "    회사이름과, 원하는 기간(시작, 마지막 날짜 포함)안의 historical data(시가, 종가, 고가, 저가, 거래량, 수정주가 적용)를\n",
    "    Daum 에서 가져온다.\n",
    "    \n",
    "    - 크롤기간의 default값은 과거~이 함수가 실행되는 timezone에서의 날짜이다.\n",
    "    - page 1 부터 requests를 날리는 구조이다.\n",
    "    - 원하는기간의 데이터가 없는 page는 그냥 넘어간다.\n",
    "    - 마지막페이지이거나, 원하는 기간내의 데이터를 이미 확보했다면 page 끝까지 requests날리지 않고 break 된다.\n",
    "    \"\"\"\n",
    "    global YEAR_CHANGED\n",
    "    # start_date, end_date 필터링 및 초기값 설정\n",
    "    if(type(start_date)== str):\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    if(type(end_date) == str):\n",
    "        end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    else:\n",
    "        end_date = end_date()\n",
    "    \n",
    "    page = 1\n",
    "    FOUND_FINALPAGE = False\n",
    "    code = db.find_stock_code(company_name)\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        req = requests.get(f'http://finance.daum.net/item/quote_yyyymmdd_sub.daum?page={page}&code={code}&modify=1')\n",
    "        html = req.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        d = len(soup.find_all('td', 'datetime2'))*5\n",
    "        p = len(soup.find_all(filter_for_history, string=is_the_only_string_within_a_tag))\n",
    "        \n",
    "        price = list(chunks([text_to_num(tag) for tag in soup.find_all(filter_for_history, string= is_the_only_string_within_a_tag)], 5))\n",
    "        date = string_to_datetime(soup.find_all('td','datetime2'))\n",
    "        \n",
    "        #cp = date.copy()\n",
    "        np_price = np.array(price)\n",
    "        np_date = np.array(date)\n",
    "        \n",
    "        index = (start_date <= np_date) * (np_date <= end_date)\n",
    "        price = np_price[index].tolist()\n",
    "        date = np_date[index].tolist()\n",
    "        \n",
    "        if((not index.sum()) and (d > 0) ):\n",
    "            #print(f\"page num: {page}\")\n",
    "            # 현재 페이지에 start_date ~ end_date 의 데이터가 없고\n",
    "            # and\n",
    "            # 현재 페이지에 (하나라도) 데이터가 있을경우\n",
    "            # page +=1 하고 이 페이지는 아무런 작없 없이 pass 한다.\n",
    "            # 하지만 이전페이지에서 원하는 데이터를 다 얻었다면 FOUND_FINALPAGE가 TRUE이므로\n",
    "            # break 된다.\n",
    "            if check_finalpage(np_date, start_date):\n",
    "                print(\"종료\")\n",
    "                YEAR_CHANGED = False\n",
    "                break\n",
    "            \n",
    "            page +=1\n",
    "            pass\n",
    "        elif(index.sum() and d>0):\n",
    "            page +=1\n",
    "            #pprint.pprint([[code, d] + p for d, p in zip(date, price)])\n",
    "            yield [[code, d] + p for d, p in zip(date, price)]\n",
    "            FOUND_FINALPAGE = check_finalpage(np_date, start_date)\n",
    "            #print(FOUND_FINALPAGE)\n",
    "            \n",
    "            if(d != 150):\n",
    "                # + 현재 페이지가 사이트에서 마지막 페이지 일경우\n",
    "                # 다음 페이지는 빈 페이지일 것 이므로 크롤 작업을 break한다.\n",
    "                # 마지막 페이지에는 항상 데이터가 30일치 이하로 있다. 그래서 d !=150조건을 썼다.\n",
    "                # 마지막 페이지가 30일치 데이터가 있으면, 마지막 페이지이지만 이 조건문에서 걸러내지 못한다.\n",
    "                # 하지만 그때는 index.sum()=0 이고 d = 0 이므로 가장 아래의 else 조건에서 break 된다.\n",
    "                YEAR_CHANGED = False\n",
    "                #print(1)\n",
    "                break\n",
    "            elif(FOUND_FINALPAGE):\n",
    "                # + 현재 페이지에서 원하는 데이터를 찾았다\n",
    "                # + 현재 페이지에 내가 원하는 날짜보다 그전날짜의 정보가있다 -> 이것이 마지막 페이지이다.\n",
    "                # ex.) 2002년 3월 10일 ~ 데이터를 원했는데 100페이지에서 해당 날짜의\n",
    "                # 데이터를 얻었다. 그러면 굳이 101페이지에 넘어가지 않고 break한다.\n",
    "                # 만약 1월 18일이 해당 페이지에서 가장 오래된 데이터일 경우는(index.sum()>0 and d>0 and check_finalpage=False)\n",
    "                # 이 조건문에서 걸러내지 못한다. 그래서 101페이지로 requests가 일어난다. 하지만 그때는 index.sum()=0, d >0 이므로\n",
    "                # 가장 첫번째 if 문으로 넘어 가게 되지만, FOUND_FINALPAGE=TRUE 라서 break 된다.\n",
    "                YEAR_CHANGED = False\n",
    "                #print(2)\n",
    "                break\n",
    "        else:\n",
    "            # 해당 페이지가 빈 페이지인 경우\n",
    "            YEAR_CHANGED = False\n",
    "            print(3)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req = requests.get('http://finance.daum.net/quote/kospi200_yyyymmdd.daum?page=1')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"datetime2\">18.03.05</td>,\n",
       " <td class=\"datetime2\">18.03.02</td>,\n",
       " <td class=\"datetime2\">18.02.28</td>,\n",
       " <td class=\"datetime2\">18.02.27</td>,\n",
       " <td class=\"datetime2\">18.02.26</td>,\n",
       " <td class=\"datetime2\">18.02.23</td>,\n",
       " <td class=\"datetime2\">18.02.22</td>,\n",
       " <td class=\"datetime2\">18.02.21</td>,\n",
       " <td class=\"datetime2\">18.02.20</td>,\n",
       " <td class=\"datetime2\">18.02.19</td>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('td', 'datetime2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"num\">66,486</td>,\n",
       " <td class=\"num\">3,451,553</td>,\n",
       " <td class=\"num\">83,585</td>,\n",
       " <td class=\"num\">4,092,223</td>,\n",
       " <td class=\"num\">94,901</td>,\n",
       " <td class=\"num\">4,933,486</td>,\n",
       " <td class=\"num\">69,195</td>,\n",
       " <td class=\"num\">3,777,421</td>,\n",
       " <td class=\"num\">86,345</td>,\n",
       " <td class=\"num\">4,003,107</td>,\n",
       " <td class=\"num\">74,326</td>,\n",
       " <td class=\"num\">3,719,487</td>,\n",
       " <td class=\"num\">65,201</td>,\n",
       " <td class=\"num\">3,251,475</td>,\n",
       " <td class=\"num\">61,420</td>,\n",
       " <td class=\"num\">3,764,450</td>,\n",
       " <td class=\"num\">56,031</td>,\n",
       " <td class=\"num\">2,967,903</td>,\n",
       " <td class=\"num\">70,084</td>,\n",
       " <td class=\"num\">4,078,959</td>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(filter_for_history, string=is_the_only_string_within_a_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = len(soup.find_all('td', 'datetime2'))*5\n",
    "p = len(soup.find_all(filter_for_history, string=is_the_only_string_within_a_tag))\n",
    "\n",
    "price = list(chunks([text_to_num(tag) for tag in soup.find_all(filter_for_history, string= is_the_only_string_within_a_tag)], 5))\n",
    "date = string_to_datetime(soup.find_all('td','datetime2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['005930',\n",
       "   datetime.datetime(2018, 3, 5, 0, 0),\n",
       "   2291000,\n",
       "   2308000,\n",
       "   2254000,\n",
       "   2260000,\n",
       "   265331],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 3, 2, 0, 0),\n",
       "   2329000,\n",
       "   2340000,\n",
       "   2300000,\n",
       "   2301000,\n",
       "   265310],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 28, 0, 0),\n",
       "   2369000,\n",
       "   2405000,\n",
       "   2350000,\n",
       "   2353000,\n",
       "   303247],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 27, 0, 0),\n",
       "   2418000,\n",
       "   2419000,\n",
       "   2369000,\n",
       "   2369000,\n",
       "   196611],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 26, 0, 0),\n",
       "   2364000,\n",
       "   2378000,\n",
       "   2354000,\n",
       "   2369000,\n",
       "   191213],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 23, 0, 0),\n",
       "   2338000,\n",
       "   2390000,\n",
       "   2338000,\n",
       "   2361000,\n",
       "   248466],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 22, 0, 0),\n",
       "   2363000,\n",
       "   2363000,\n",
       "   2338000,\n",
       "   2338000,\n",
       "   177399],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 21, 0, 0),\n",
       "   2364000,\n",
       "   2379000,\n",
       "   2342000,\n",
       "   2364000,\n",
       "   257604],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 20, 0, 0),\n",
       "   2402000,\n",
       "   2408000,\n",
       "   2361000,\n",
       "   2370000,\n",
       "   202452],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 19, 0, 0),\n",
       "   2490000,\n",
       "   2490000,\n",
       "   2393000,\n",
       "   2419000,\n",
       "   307069],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 14, 0, 0),\n",
       "   2404000,\n",
       "   2455000,\n",
       "   2397000,\n",
       "   2450000,\n",
       "   378118],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 13, 0, 0),\n",
       "   2310000,\n",
       "   2403000,\n",
       "   2310000,\n",
       "   2377000,\n",
       "   378465],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 12, 0, 0),\n",
       "   2255000,\n",
       "   2316000,\n",
       "   2252000,\n",
       "   2286000,\n",
       "   315099],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 9, 0, 0),\n",
       "   2222000,\n",
       "   2259000,\n",
       "   2221000,\n",
       "   2235000,\n",
       "   349300],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 8, 0, 0),\n",
       "   2306000,\n",
       "   2331000,\n",
       "   2299000,\n",
       "   2300000,\n",
       "   465021],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 7, 0, 0),\n",
       "   2412000,\n",
       "   2413000,\n",
       "   2290000,\n",
       "   2290000,\n",
       "   468961],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 6, 0, 0),\n",
       "   2330000,\n",
       "   2396000,\n",
       "   2329000,\n",
       "   2371000,\n",
       "   388129],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 5, 0, 0),\n",
       "   2325000,\n",
       "   2416000,\n",
       "   2300000,\n",
       "   2396000,\n",
       "   567158],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 2, 0, 0),\n",
       "   2469000,\n",
       "   2470000,\n",
       "   2385000,\n",
       "   2385000,\n",
       "   585207],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 2, 1, 0, 0),\n",
       "   2531000,\n",
       "   2548000,\n",
       "   2486000,\n",
       "   2491000,\n",
       "   552189],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 31, 0, 0),\n",
       "   2501000,\n",
       "   2707000,\n",
       "   2480000,\n",
       "   2495000,\n",
       "   1293626],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 30, 0, 0),\n",
       "   2522000,\n",
       "   2532000,\n",
       "   2489000,\n",
       "   2490000,\n",
       "   245691],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 29, 0, 0),\n",
       "   2560000,\n",
       "   2574000,\n",
       "   2545000,\n",
       "   2561000,\n",
       "   236776],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 26, 0, 0),\n",
       "   2525000,\n",
       "   2539000,\n",
       "   2492000,\n",
       "   2539000,\n",
       "   207002],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 25, 0, 0),\n",
       "   2461000,\n",
       "   2518000,\n",
       "   2458000,\n",
       "   2513000,\n",
       "   223207],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 24, 0, 0),\n",
       "   2443000,\n",
       "   2485000,\n",
       "   2428000,\n",
       "   2467000,\n",
       "   191010],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 23, 0, 0),\n",
       "   2433000,\n",
       "   2458000,\n",
       "   2415000,\n",
       "   2458000,\n",
       "   270654],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 22, 0, 0),\n",
       "   2432000,\n",
       "   2434000,\n",
       "   2398000,\n",
       "   2412000,\n",
       "   250418],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 19, 0, 0),\n",
       "   2519000,\n",
       "   2519000,\n",
       "   2452000,\n",
       "   2466000,\n",
       "   184399],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 18, 0, 0),\n",
       "   2501000,\n",
       "   2532000,\n",
       "   2491000,\n",
       "   2495000,\n",
       "   296977]],\n",
       " [['005930',\n",
       "   datetime.datetime(2018, 1, 17, 0, 0),\n",
       "   2501000,\n",
       "   2501000,\n",
       "   2453000,\n",
       "   2481000,\n",
       "   221061],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 16, 0, 0),\n",
       "   2438000,\n",
       "   2507000,\n",
       "   2431000,\n",
       "   2500000,\n",
       "   407793],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 15, 0, 0),\n",
       "   2440000,\n",
       "   2449000,\n",
       "   2396000,\n",
       "   2427000,\n",
       "   201920],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 12, 0, 0),\n",
       "   2412000,\n",
       "   2424000,\n",
       "   2338000,\n",
       "   2410000,\n",
       "   545409],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 11, 0, 0),\n",
       "   2410000,\n",
       "   2463000,\n",
       "   2401000,\n",
       "   2412000,\n",
       "   502476],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 10, 0, 0),\n",
       "   2525000,\n",
       "   2526000,\n",
       "   2432000,\n",
       "   2442000,\n",
       "   371336],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 9, 0, 0),\n",
       "   2573000,\n",
       "   2586000,\n",
       "   2499000,\n",
       "   2520000,\n",
       "   360272],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 8, 0, 0),\n",
       "   2620000,\n",
       "   2626000,\n",
       "   2575000,\n",
       "   2601000,\n",
       "   167673],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 5, 0, 0),\n",
       "   2565000,\n",
       "   2606000,\n",
       "   2560000,\n",
       "   2606000,\n",
       "   189623],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 4, 0, 0),\n",
       "   2606000,\n",
       "   2609000,\n",
       "   2532000,\n",
       "   2554000,\n",
       "   233909],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 3, 0, 0),\n",
       "   2627000,\n",
       "   2628000,\n",
       "   2571000,\n",
       "   2581000,\n",
       "   200270],\n",
       "  ['005930',\n",
       "   datetime.datetime(2018, 1, 2, 0, 0),\n",
       "   2569000,\n",
       "   2570000,\n",
       "   2539000,\n",
       "   2551000,\n",
       "   169485],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 28, 0, 0),\n",
       "   2478000,\n",
       "   2548000,\n",
       "   2475000,\n",
       "   2548000,\n",
       "   179709],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 27, 0, 0),\n",
       "   2448000,\n",
       "   2478000,\n",
       "   2423000,\n",
       "   2468000,\n",
       "   214872],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 26, 0, 0),\n",
       "   2488000,\n",
       "   2505000,\n",
       "   2410000,\n",
       "   2410000,\n",
       "   320797],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 22, 0, 0),\n",
       "   2470000,\n",
       "   2498000,\n",
       "   2462000,\n",
       "   2485000,\n",
       "   223993],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 21, 0, 0),\n",
       "   2550000,\n",
       "   2553000,\n",
       "   2455000,\n",
       "   2457000,\n",
       "   312486],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 20, 0, 0),\n",
       "   2575000,\n",
       "   2588000,\n",
       "   2541000,\n",
       "   2544000,\n",
       "   201611],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 19, 0, 0),\n",
       "   2577000,\n",
       "   2604000,\n",
       "   2576000,\n",
       "   2578000,\n",
       "   239572],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 18, 0, 0),\n",
       "   2531000,\n",
       "   2562000,\n",
       "   2531000,\n",
       "   2560000,\n",
       "   147005],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 15, 0, 0),\n",
       "   2562000,\n",
       "   2574000,\n",
       "   2526000,\n",
       "   2531000,\n",
       "   298571],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 14, 0, 0),\n",
       "   2566000,\n",
       "   2614000,\n",
       "   2553000,\n",
       "   2553000,\n",
       "   406208],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 13, 0, 0),\n",
       "   2605000,\n",
       "   2605000,\n",
       "   2555000,\n",
       "   2566000,\n",
       "   226674],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 12, 0, 0),\n",
       "   2591000,\n",
       "   2605000,\n",
       "   2583000,\n",
       "   2605000,\n",
       "   175301],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 11, 0, 0),\n",
       "   2600000,\n",
       "   2602000,\n",
       "   2575000,\n",
       "   2589000,\n",
       "   162517],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 8, 0, 0),\n",
       "   2568000,\n",
       "   2600000,\n",
       "   2552000,\n",
       "   2600000,\n",
       "   225809],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 7, 0, 0),\n",
       "   2502000,\n",
       "   2549000,\n",
       "   2501000,\n",
       "   2537000,\n",
       "   221312],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 6, 0, 0),\n",
       "   2563000,\n",
       "   2578000,\n",
       "   2501000,\n",
       "   2501000,\n",
       "   217784],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 5, 0, 0),\n",
       "   2530000,\n",
       "   2565000,\n",
       "   2514000,\n",
       "   2563000,\n",
       "   186531],\n",
       "  ['005930',\n",
       "   datetime.datetime(2017, 12, 4, 0, 0),\n",
       "   2542000,\n",
       "   2567000,\n",
       "   2501000,\n",
       "   2567000,\n",
       "   297301]],\n",
       " [['005930',\n",
       "   datetime.datetime(2017, 12, 1, 0, 0),\n",
       "   2540000,\n",
       "   2589000,\n",
       "   2540000,\n",
       "   2542000,\n",
       "   257372]]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(base_crawl_method_for_history('삼성전자', \"2017-12-1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 인자: 회사명, 시작날짜, 마지막날짜, 에코모드\n",
    "\n",
    "# 에코모드가 아닐경우\n",
    "# 기간내의 데이터를 크롤해서 yield\n",
    "\n",
    "\n",
    "# 에코모드일경우\n",
    "# 원하는 기간 중 이미 DB에 데이터가있는 날짜를 제외한 \n",
    "# 기간에대해서 크롤을 한다.\n",
    "\n",
    "# 디비에 데이터가 있는지 없는지 확인한다.\n",
    "#     있는 경우:\n",
    "#         디비에서가장오래된날짜\n",
    "#         디비에서가장최근날짜 들을 구한다.\n",
    "#         \n",
    "#     없는 경우:\n",
    "# 에코모드시 작동\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_target_item(dic, col_name):\n",
    "    dic['target_'+col_name] = dic.get(col_name)\n",
    "    return dic\n",
    "\n",
    "def update_multiple(table_name, column, data_dict_list):\n",
    "    \"\"\"\n",
    "    하나의 column에 대해서 조건을 줘서\n",
    "    DB에 기존에 있던 데이터에 대해서 한 번에 Update 한다.\n",
    "    \"\"\"\n",
    "    transaction = db._BaseDBAccessLayer__connection.begin() #self.__connection.begin()\n",
    "\n",
    "    if(data_dict_list):\n",
    "        try:\n",
    "            table = db.find_table(table_name)\n",
    "            if(table.c.has_key(column)):\n",
    "                c = table.c.get(column)\n",
    "                \n",
    "                upd = table.update().\\\n",
    "                      where(c == bindparam('target_'+column))\n",
    "                    \n",
    "                added_data_dict_list = [add_target_item(dic, column) for dic in data_dict_list]\n",
    "                #print(\"added_data_dict_list: \")\n",
    "                #print(added_data_dict_list)\n",
    "                result = db._BaseDBAccessLayer__connection.execute(upd, added_data_dict_list)\n",
    "                transaction.commit()\n",
    "                return True\n",
    "                      \n",
    "            else:\n",
    "                print(\"{} table 에는 {} Column 이 없습니다\".format(table_name, column))\n",
    "                print(\"Let's RollBack\")\n",
    "                transaction.rollback()\n",
    "                return False\n",
    "\n",
    "        except IntegrityError as error:\n",
    "            error_code, error_msg = error.orig.args\n",
    "\n",
    "            print(\"IntegrityError!!\")\n",
    "            print(\">>>error_code: {}\".format(error_code))\n",
    "            print(\">>>error_msg: {}\".format(error_msg))\n",
    "            print(\"Let's RollBack\")\n",
    "            transaction.rollback()\n",
    "\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        print(\"빈 dict입니다\")\n",
    "        transaction.rollback()\n",
    "        print(\"Let's RollBack\")\n",
    "        return False\n",
    "\n",
    "    \n",
    "def check_intersection(t1_start, t1_end, t2_start, t2_end):\n",
    "    return (t1_start <= t2_start <= t1_end) or (t2_start <= t1_start <= t2_end)\n",
    "\n",
    "\n",
    "def toss_databomb_to_db(bigdata, update_mode=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    data_count = 0\n",
    "    total_count = 0\n",
    "    historical_data_list = []\n",
    "    is_okay = True\n",
    "    \n",
    "    \n",
    "    if(update_mode):\n",
    "        # UPDATE\n",
    "        for data in bigdata:\n",
    "            # 이때 data는 2차원 list 다.\n",
    "            # ex)\n",
    "            #[['005930', datetime.datetime(2018, 2, 26, 0, 0),\n",
    "            #  2364000, 2378000,\n",
    "            #  2354000, 2369000,\n",
    "            #  163980],\n",
    "            # ['005930', datetime.datetime(2018, 2, 23, 0, 0),\n",
    "            #  2338000, 2390000,\n",
    "            #  2338000, 2361000,\n",
    "            #  248466]]\n",
    "            print(\">>>toss 안\")\n",
    "            print(\"Data: \", data)\n",
    "            data_count += len(data)\n",
    "            print(\"Data_count: \", data_count)\n",
    "            historical_data_list += [listdata_to_dict(oneday_info) for oneday_info in data]\n",
    "\n",
    "            # 100도 나중에 상수로 바꿔야한다.\n",
    "            if(data_count >= 100):\n",
    "                total_count += data_count\n",
    "                data_count = 0\n",
    "                print(\"historical: \", historical_data_list)\n",
    "                is_okay = update_multiple('hist_data', 'date', historical_data_list)\n",
    "                historical_data_list = []\n",
    "\n",
    "        total_count += data_count\n",
    "        print(\"historical: \", historical_data_list)\n",
    "        is_okay *= update_multiple('hist_data', 'date', historical_data_list)\n",
    "    \n",
    "    else:\n",
    "        # INSERT\n",
    "        for data in bigdata:\n",
    "            data_count += len(data)\n",
    "            historical_data_list += [listdata_to_dict(oneday_info) for oneday_info in data]\n",
    "\n",
    "            if(data_count >= 100):\n",
    "                total_count += data_count\n",
    "                data_count = 0\n",
    "                is_okay = db.insert_multiple('hist_data', historical_data_list)\n",
    "                historical_data_list = []\n",
    "\n",
    "        is_okay *= db.insert_multiple('hist_data', historical_data_list)\n",
    "        total_count += data_count\n",
    "    \n",
    "    return is_okay, total_count\n",
    "\n",
    "# case1: 현재 DB는 비었고, 전체기간[과거부터~오늘(최신날짜)] 주가 정보를 가져와서 DB에 저장한다.\n",
    "#     -> 새 환경에서 첫번째로 수행될 작업\n",
    "# case2: 현재 DB는 비었고, 특정기간[start_date, end_date] 주가 정보를 가져와서 DB에 저장한다.\n",
    "# case3: 현재 DB에는 데이터가 있는 상황, 추가된 데이터(db속최신날짜 이후 ~ 크롤시점) 를 가져와서 DB에 저장하는 경우\n",
    "#     -> case1후 매일 크롤러가 수행할 작업 \n",
    "# case4: 현재 DB에는 데이터가 있는 상황, 수정주가로 UPDATE해야되는 상황\n",
    "#     -> 배당, 액면분할 등 의 event로 인해 주가가 update되어야 할 경우 가끔식 수행\n",
    "\n",
    "# save 경우\n",
    "# HARD 모드인지 ,DB에 데이터가 있는지 조건확인\n",
    "# 1. 해당 기간의 데이터가 DB에 있는지 없는지 상관없이 원하는 기간내의 데이터를 새로 크롤해와서 DB에 저장하는경우(UPDATE, INSERT 둘 다)\n",
    "#  case4\n",
    "#  - DB에 있는 날짜의 데이터의 경우에는 가격정보를 UPDATE\n",
    "#        - DB에 데이터가 있는지 확인, 교집합이 있는지 확인\n",
    "#        - 날짜 교집합을 만들어서 start, end date를 정한다.\n",
    "#        - 크롤한다.\n",
    "#        - UPDATE 한다.\n",
    "#  - DB에 없는 날짜의 데이터의 경우에는 가격정보를 INSERT\n",
    "#        - DB에 없는 기간 인덱스를 만든다.\n",
    "#        - start, end date를 정한다.\n",
    "#        - 크롤한다.\n",
    "#        - INSERT 한다.\n",
    "#  - DB에 없는 기간을 선택하기 위해서 만드는 index가 여러개일 수 있다. base_crwaler_method 여러번 돌려야 할 수 있다.\n",
    "\n",
    "# 2. 해당 기간의 데이터가 있을 경우 그 기간은 제외하고 DB에 없는 기간의 데이터만 크롤해서 DB에 저장한다.(INSERT만 일어남)\n",
    "#    case1, case2, case3\n",
    "#  - DB에 없는 기간을 선택하기 위해서 만드는 index가 여러개일 수 있다. base_crwaler_method 여러번 돌려야 할 수 있다.\n",
    "#  - 날짜의 교집합을 제외한 날짜 index을 만든다. start, end date 를 정한다.\n",
    "#  - 크롤한다.\n",
    "#  - INSERT 한다.\n",
    "def save_history_of(company_name, start_date=datetime.datetime.min,\n",
    "                    end_date=datetime.datetime.today,\n",
    "                    hard_mode=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    total_count = 0\n",
    "    update_count = 0\n",
    "    insert_count = 0\n",
    "    update_only = False\n",
    "    \n",
    "    # start_date, end_date 필터링 및 초기값 설정\n",
    "    if(type(start_date)== str):\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    if(type(end_date) == str):\n",
    "        end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    else:\n",
    "        end_date = end_date()\n",
    "\n",
    "        \n",
    "    table = db.find_table('hist_data') # 클래스에서 바꿔야함. \n",
    "    \n",
    "    oldest_date_indb_query = select([table.c.get('date')]).order_by(table.c.date).limit(1)\n",
    "    lastest_date_indb_query = select([table.c.get('date')]).order_by(desc(table.c.date)).limit(1)\n",
    "    \n",
    "    oldest_rp = db.db_execute(oldest_date_indb_query)\n",
    "    lastest_rp = db.db_execute(lastest_date_indb_query)\n",
    "    \n",
    "    # DB에 데이터가 있을 때 hard_mode여 부에 따라서 UPDATE 와 INSERT\n",
    "    if(oldest_rp.rowcount):\n",
    "        # lastest_indb = db에 저장된 데이터중 가장 최근 날짜의 datetime을 가져온다.\n",
    "        # oldest_date_indb = db에 저장된 데이터중 가장 오래된 날짜의 datetime을 가져온다.\n",
    "        oldest_date_indb = oldest_rp.first()[0]\n",
    "        lastest_date_indb = lastest_rp.first()[0]\n",
    "\n",
    "        # 두 기간의 intersection이 있는지 확인한다.\n",
    "        if(check_intersection(oldest_date_indb, lastest_date_indb, start_date, end_date)):\n",
    "            # 두 기간에대해서 time interval list(datetime)를 만든다.\n",
    "            db_time_interval= set(map(pd.Timestamp.to_pydatetime, pd.date_range(oldest_date_indb, lastest_date_indb).tolist()))\n",
    "            request_time_interval= set(map(pd.Timestamp.to_pydatetime, pd.date_range(start_date, end_date).tolist()))\n",
    "\n",
    "            # db 데이터의 interval과 원하는 interval의 교집합을 구한다.\n",
    "            intersection = db_time_interval & request_time_interval\n",
    "            intersection_as_list = list(intersection)\n",
    "            intersection_as_list.sort()\n",
    "            intersection_start_date = intersection_as_list[0]\n",
    "            intersection_end_date = intersection_as_list[-1]\n",
    "\n",
    "            # intersection 구간에서는 hard_mode인 경우는 UPDATE\n",
    "            # hard_mode가 아닌 경우는 pass\n",
    "            if(hard_mode):\n",
    "                print(\"요기요\")\n",
    "                print(\"inter_start: \", intersection_start_date)\n",
    "                print(\"inter_end:   \", intersection_end_date)\n",
    "                data_bomb = base_crawl_method_for_history(company_name, intersection_start_date.strftime(\"%Y-%m-%d\")\n",
    "                                                          , intersection_end_date.strftime(\"%Y-%m-%d\"))\n",
    "                \n",
    "                update_okay, update_count = toss_databomb_to_db(data_bomb, update_mode=True)\n",
    "                if(update_okay):\n",
    "                    print(f\"{start_date} ~ {end_date} 기간의 \")\n",
    "                    print(f\"데이터가 크롤 및 UPDATE 되었습니다. UPDATE COUNT: {update_count} \")\n",
    "                else:\n",
    "                    print(\"intersection 데이터에 대해서 UPDATE가 정상적으로 이루어 지지 않음\")\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            \n",
    "            # unique 기간에대해서는 INSERT\n",
    "            \n",
    "            unique_req = request_time_interval - intersection\n",
    "            unique_db = db_time_interval - intersection\n",
    "            unique_req_as_list = list(unique_req)\n",
    "            unique_req_as_list.sort()\n",
    "            \n",
    "            if(len(unique_req) == 0):\n",
    "                # 이미 intersection 에서 처리됨. 그러므로 pass함.\n",
    "                # case1: DB기간안에 start~end가 포함되는 경우\n",
    "                # case1: DB기간과 start-end가 같은경우\n",
    "                if(hard_mode==False):\n",
    "                    print(\"hard_mode 설정이 False 이고\")\n",
    "                    print(\"요청하신 기간의 데이터는 이미 DB에 있으니\")\n",
    "                    print(\"아무런 작업이 일어나지 않습니다.\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(\"update_only\")\n",
    "                    update_only = True\n",
    "                insert_okay = False\n",
    "                pass\n",
    "            elif((len(unique_req) > 0) and (start_date < oldest_date_indb) and (lastest_date_indb < end_date) \\\n",
    "                 and (len(unique_db) == 0) ):\n",
    "                # start_date~end_date 가 DB기간을 포함하면서 더 넓은 경우, 최대 양쪽 끝 두 범위가 나올 수 있음.\n",
    "                # 이런 경우는 드물 것이라고 예상.\n",
    "                # 양쪽 다 나온 경우 : 왼쪽, 오른쪽 unique 범위에대해서 처리.\n",
    "                print(\"unique_req\")\n",
    "                print(unique_req)\n",
    "                print(\"unique_db\")\n",
    "                print(unique_db)\n",
    "                left_side = [date for date in unique_req_as_list if date < intersection_start_date]\n",
    "                left_side_start = left_side[0]\n",
    "                left_side_end = left_side[-1]\n",
    "\n",
    "                data_bomb = base_crawl_method_for_history(company_name, left_start_date.strftime(\"%Y-%m-%d\")\n",
    "                                                          , left_end_date.strftime(\"%Y-%m-%d\"))\n",
    "                left_okay, left_count = toss_databomb_to_db(data_bomb, update_mode=False)\n",
    "                if(left_okay):\n",
    "                    print(f\"{left_start_date} ~ {left_end_date} 기간의 \")\n",
    "                    print(f\"데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: {left_count} \")\n",
    "                else:\n",
    "                    print(f\"{left_start_date} ~ {left_end_date} 기간의 \")\n",
    "                    print(f\"데이터가 정상적으로 크롤 및 INSERT 되지 않았습니다. INSERT COUNT: {left_count} \")\n",
    "                right_side = [ date for date in unique_req_as_list if date > intersection_end_date ]\n",
    "                right_side_start = right_side[0]\n",
    "                right_side_end = right_side[-1]\n",
    "\n",
    "                data_bomb = base_crawl_method_for_history(company_name, right_start_date.strftime(\"%Y-%m-%d\")\n",
    "                                                          ,right_end_date.strftime(\"%Y-%m-%d\"))\n",
    "                right_okay, right_count = toss_databomb_to_db(data_bomb, update_mode=False)\n",
    "                if(right_okay):\n",
    "                    print(f\"{right_start_date} ~ {right_end_date} 기간의 \")\n",
    "                    print(f\"데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: {right_count} \")\n",
    "                else:\n",
    "                    print(f\"{right_start_date} ~ {right_end_date} 기간의 \")\n",
    "                    print(f\"데이터가 정상적으로 크롤 및 INSERT 되지 않았습니다. INSERT COUNT: {right_count} \")\n",
    "                \n",
    "                insert_okay = left_okay * right_okay\n",
    "                insert_count = left_count + right_count\n",
    "                \n",
    "            else:\n",
    "                # \n",
    "                # 왼쪽 또는 오른쪽만 나온 경우\n",
    "                unique_req_start_date = unique_req_as_list[0]\n",
    "                unique_req_end_date = unique_req_as_list[-1]\n",
    "\n",
    "                data_bomb = base_crawl_method_for_history(company_name, unique_req_start_date.strftime(\"%Y-%m-%d\")\n",
    "                                                          ,unique_req_end_date.strftime(\"%Y-%m-%d\"))\n",
    "                insert_okay, insert_count = toss_databomb_to_db(data_bomb, update_mode=False)\n",
    "                if(insert_okay):\n",
    "                    print(f\"{unique_req_start_date} ~ {unique_req_end_date} 기간의 \")\n",
    "                    print(f\"데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: {insert_count} \")\n",
    "                else:\n",
    "                    print(f\"{unique_req_start_date} ~ {unique_req_end_date} 기간의 \")\n",
    "                    print(f\"데이터가 정상적으로 크롤 및 INSERT 되지 않았습니다. INSERT COUNT: {insert_count} \")\n",
    "            \n",
    "            \n",
    "        # 두 기간의 intersection이 없을 때\n",
    "        else:\n",
    "            # start_date~end_date기간의 데이터에 대해서 INSERT만 일어나면된다.\n",
    "            data_bomb = base_crawl_method_for_history(company_name, start_date.strftime(\"%Y-%m-%d\")\n",
    "                                                      , end_date.strftime(\"%Y-%m-%d\"))\n",
    "            insert_okay, insert_count = toss_databomb_to_db(data_bomb, update_mode=False)\n",
    "            if(insert_okay):\n",
    "                print(\"DB기간과 겹치는 기간이 없었습니다.\")\n",
    "                print(f\"{start_date} ~ {end_date} 기간의 \")\n",
    "                print(f\"데이터가 크롤 및 저장 되었습니다 INSERT COUNT: {insert_count}\")\n",
    "            else:\n",
    "                print(\"DB기간과 겹치는 기간이 없었습니다.\")\n",
    "                print(\"하지만\")\n",
    "                print(f\"{start_date} ~ {end_date} 기간의 \")\n",
    "                print(\"데이터가 정상적으로 INSERT가 이루어 지지 않았습니다.\")\n",
    "        \n",
    "        # INSERT 마무리\n",
    "        if(insert_okay and not update_only):\n",
    "            print(f\"총 {insert_count}개의 데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: {insert_count} \")\n",
    "            return True\n",
    "        elif(update_only):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    # DB에 데이터가 없을때 start_date~end_date에 대해서 INSERT\n",
    "    else:\n",
    "        # DB에 데이터가 없을 때는 \n",
    "        # \n",
    "        # hard_mode인지 아닌지가 필요 없다. \n",
    "        data_bomb = base_crawl_method_for_history(company_name, start_date.strftime(\"%Y-%m-%d\")\n",
    "                                                  , end_date.strftime(\"%Y-%m-%d\"))\n",
    "        insert_okay, insert_count = toss_databomb_to_db(data_bomb, update_mode=False)\n",
    "        \n",
    "        if(insert_okay):\n",
    "            print(\"DB에 데이터가 아무것도 없습니다.\")\n",
    "            print(f\"{start_date} ~ {end_date} 기간의 \")\n",
    "            print(f\"INSERT COUNT: {insert_count} 데이터가 크롤 및 저장 되었습니다.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"DB에 데이터가 아무것도 없습니다.\")\n",
    "            print(\"하지만\")\n",
    "            print(f\"{start_date} ~ {end_date} 기간의 \")\n",
    "            print(\"데이터가 정상적으로 INSERT가 이루어 지지 않았습니다.\")\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'005930'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.find_stock_code('삼성전자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['005930',\n",
      "  datetime.datetime(2018, 2, 27, 0, 0),\n",
      "  2418000,\n",
      "  2419000,\n",
      "  2369000,\n",
      "  2369000,\n",
      "  188292]]\n",
      "2018-02-27 00:00:00 ~ 2018-02-27 00:00:00 기간의 \n",
      "데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: 1 \n",
      "총 1개의 데이터가 크롤 및 INSERT 되었습니다. UPDATE COUNT: 0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_history_of('삼성전자', \"2018-2-20\", \"2018-2-27\", hard_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard_mode 설정이 False 이고\n",
      "요청하신 기간의 데이터는 이미 DB에 있으니\n",
      "아무런 작업이 일어나지 않습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_history_of('삼성전자', \"2018-2-20\", \"2018-2-27\", hard_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-20 00:00:00 ~ 2018-02-27 00:00:00 기간의 \n",
      "데이터가 크롤 및 UPDATE 되었습니다. UPDATE COUNT: 6 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_history_of('삼성전자', \"2018-2-20\", \"2018-2-27\", hard_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------99년 발견------------\n",
      "DB기간과 겹치는 기간이 없었습니다.\n",
      "1999-12-20 00:00:00 ~ 2000-01-10 00:00:00 기간의 \n",
      "데이터가 크롤 및 저장 되었습니다 INSERT COUNT: 12\n",
      "총 12개의 데이터가 크롤 및 INSERT 되었습니다. INSERT COUNT: 12 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_history_of('삼성전자', \"1999-12-20\", \"2000-1-10\", hard_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------99년 발견------------\n",
      "1999-12-20 00:00:00 ~ 2000-01-29 00:00:00 기간의 \n",
      "데이터가 크롤 및 UPDATE 되었습니다. UPDATE COUNT: 26 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_history_of('삼성전자', \"1999-12-20\", \"2000-1-29\", hard_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요기요\n",
      "inter_start:  2000-01-05 00:00:00\n",
      "inter_end:    2000-01-29 00:00:00\n",
      "[['005930',\n",
      "  datetime.datetime(2000, 1, 28, 0, 0),\n",
      "  282000,\n",
      "  294000,\n",
      "  277000,\n",
      "  291000,\n",
      "  766398],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 27, 0, 0),\n",
      "  274000,\n",
      "  281000,\n",
      "  271500,\n",
      "  276000,\n",
      "  558083],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 26, 0, 0),\n",
      "  275000,\n",
      "  276000,\n",
      "  270500,\n",
      "  274000,\n",
      "  585734],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 25, 0, 0),\n",
      "  276000,\n",
      "  282000,\n",
      "  272000,\n",
      "  272000,\n",
      "  885067],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 24, 0, 0),\n",
      "  290000,\n",
      "  295000,\n",
      "  285000,\n",
      "  285000,\n",
      "  687183],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 21, 0, 0),\n",
      "  297500,\n",
      "  299000,\n",
      "  294000,\n",
      "  294000,\n",
      "  650728],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 20, 0, 0),\n",
      "  293000,\n",
      "  302000,\n",
      "  291000,\n",
      "  302000,\n",
      "  746360],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 19, 0, 0),\n",
      "  300000,\n",
      "  302000,\n",
      "  298000,\n",
      "  298000,\n",
      "  804159],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 18, 0, 0),\n",
      "  308000,\n",
      "  308000,\n",
      "  299000,\n",
      "  305000,\n",
      "  905231],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 17, 0, 0),\n",
      "  300000,\n",
      "  309000,\n",
      "  296000,\n",
      "  305000,\n",
      "  1270138],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 14, 0, 0),\n",
      "  286000,\n",
      "  294000,\n",
      "  284000,\n",
      "  291500,\n",
      "  987576],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 13, 0, 0),\n",
      "  280000,\n",
      "  287000,\n",
      "  278000,\n",
      "  285500,\n",
      "  823830],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 12, 0, 0),\n",
      "  280500,\n",
      "  287000,\n",
      "  280000,\n",
      "  286000,\n",
      "  584492],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 11, 0, 0),\n",
      "  291000,\n",
      "  305000,\n",
      "  288500,\n",
      "  288500,\n",
      "  1194974],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 10, 0, 0),\n",
      "  280000,\n",
      "  288500,\n",
      "  279000,\n",
      "  288500,\n",
      "  937615]]\n",
      ">>>toss 안\n",
      "Data:  [['005930', datetime.datetime(2000, 1, 28, 0, 0), 282000, 294000, 277000, 291000, 766398], ['005930', datetime.datetime(2000, 1, 27, 0, 0), 274000, 281000, 271500, 276000, 558083], ['005930', datetime.datetime(2000, 1, 26, 0, 0), 275000, 276000, 270500, 274000, 585734], ['005930', datetime.datetime(2000, 1, 25, 0, 0), 276000, 282000, 272000, 272000, 885067], ['005930', datetime.datetime(2000, 1, 24, 0, 0), 290000, 295000, 285000, 285000, 687183], ['005930', datetime.datetime(2000, 1, 21, 0, 0), 297500, 299000, 294000, 294000, 650728], ['005930', datetime.datetime(2000, 1, 20, 0, 0), 293000, 302000, 291000, 302000, 746360], ['005930', datetime.datetime(2000, 1, 19, 0, 0), 300000, 302000, 298000, 298000, 804159], ['005930', datetime.datetime(2000, 1, 18, 0, 0), 308000, 308000, 299000, 305000, 905231], ['005930', datetime.datetime(2000, 1, 17, 0, 0), 300000, 309000, 296000, 305000, 1270138], ['005930', datetime.datetime(2000, 1, 14, 0, 0), 286000, 294000, 284000, 291500, 987576], ['005930', datetime.datetime(2000, 1, 13, 0, 0), 280000, 287000, 278000, 285500, 823830], ['005930', datetime.datetime(2000, 1, 12, 0, 0), 280500, 287000, 280000, 286000, 584492], ['005930', datetime.datetime(2000, 1, 11, 0, 0), 291000, 305000, 288500, 288500, 1194974], ['005930', datetime.datetime(2000, 1, 10, 0, 0), 280000, 288500, 279000, 288500, 937615]]\n",
      "Data_count:  15\n",
      "--------99년 발견------------\n",
      "[['005930',\n",
      "  datetime.datetime(2000, 1, 7, 0, 0),\n",
      "  278000,\n",
      "  283500,\n",
      "  268000,\n",
      "  277000,\n",
      "  806195],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 6, 0, 0),\n",
      "  287500,\n",
      "  289000,\n",
      "  279000,\n",
      "  281000,\n",
      "  1087810],\n",
      " ['005930',\n",
      "  datetime.datetime(2000, 1, 5, 0, 0),\n",
      "  290000,\n",
      "  303000,\n",
      "  276000,\n",
      "  279000,\n",
      "  1493604]]\n",
      ">>>toss 안\n",
      "Data:  [['005930', datetime.datetime(2000, 1, 7, 0, 0), 278000, 283500, 268000, 277000, 806195], ['005930', datetime.datetime(2000, 1, 6, 0, 0), 287500, 289000, 279000, 281000, 1087810], ['005930', datetime.datetime(2000, 1, 5, 0, 0), 290000, 303000, 276000, 279000, 1493604]]\n",
      "Data_count:  18\n",
      "historical:  [{'code': '005930', 'date': datetime.datetime(2000, 1, 28, 0, 0), 'open': 282000, 'high': 294000, 'low': 277000, 'close': 291000, 'volume': 766398}, {'code': '005930', 'date': datetime.datetime(2000, 1, 27, 0, 0), 'open': 274000, 'high': 281000, 'low': 271500, 'close': 276000, 'volume': 558083}, {'code': '005930', 'date': datetime.datetime(2000, 1, 26, 0, 0), 'open': 275000, 'high': 276000, 'low': 270500, 'close': 274000, 'volume': 585734}, {'code': '005930', 'date': datetime.datetime(2000, 1, 25, 0, 0), 'open': 276000, 'high': 282000, 'low': 272000, 'close': 272000, 'volume': 885067}, {'code': '005930', 'date': datetime.datetime(2000, 1, 24, 0, 0), 'open': 290000, 'high': 295000, 'low': 285000, 'close': 285000, 'volume': 687183}, {'code': '005930', 'date': datetime.datetime(2000, 1, 21, 0, 0), 'open': 297500, 'high': 299000, 'low': 294000, 'close': 294000, 'volume': 650728}, {'code': '005930', 'date': datetime.datetime(2000, 1, 20, 0, 0), 'open': 293000, 'high': 302000, 'low': 291000, 'close': 302000, 'volume': 746360}, {'code': '005930', 'date': datetime.datetime(2000, 1, 19, 0, 0), 'open': 300000, 'high': 302000, 'low': 298000, 'close': 298000, 'volume': 804159}, {'code': '005930', 'date': datetime.datetime(2000, 1, 18, 0, 0), 'open': 308000, 'high': 308000, 'low': 299000, 'close': 305000, 'volume': 905231}, {'code': '005930', 'date': datetime.datetime(2000, 1, 17, 0, 0), 'open': 300000, 'high': 309000, 'low': 296000, 'close': 305000, 'volume': 1270138}, {'code': '005930', 'date': datetime.datetime(2000, 1, 14, 0, 0), 'open': 286000, 'high': 294000, 'low': 284000, 'close': 291500, 'volume': 987576}, {'code': '005930', 'date': datetime.datetime(2000, 1, 13, 0, 0), 'open': 280000, 'high': 287000, 'low': 278000, 'close': 285500, 'volume': 823830}, {'code': '005930', 'date': datetime.datetime(2000, 1, 12, 0, 0), 'open': 280500, 'high': 287000, 'low': 280000, 'close': 286000, 'volume': 584492}, {'code': '005930', 'date': datetime.datetime(2000, 1, 11, 0, 0), 'open': 291000, 'high': 305000, 'low': 288500, 'close': 288500, 'volume': 1194974}, {'code': '005930', 'date': datetime.datetime(2000, 1, 10, 0, 0), 'open': 280000, 'high': 288500, 'low': 279000, 'close': 288500, 'volume': 937615}, {'code': '005930', 'date': datetime.datetime(2000, 1, 7, 0, 0), 'open': 278000, 'high': 283500, 'low': 268000, 'close': 277000, 'volume': 806195}, {'code': '005930', 'date': datetime.datetime(2000, 1, 6, 0, 0), 'open': 287500, 'high': 289000, 'low': 279000, 'close': 281000, 'volume': 1087810}, {'code': '005930', 'date': datetime.datetime(2000, 1, 5, 0, 0), 'open': 290000, 'high': 303000, 'low': 276000, 'close': 279000, 'volume': 1493604}]\n",
      "added_data_dict_list: \n",
      "[{'code': '005930', 'date': datetime.datetime(2000, 1, 28, 0, 0), 'open': 282000, 'high': 294000, 'low': 277000, 'close': 291000, 'volume': 766398, 'target_date': datetime.datetime(2000, 1, 28, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 27, 0, 0), 'open': 274000, 'high': 281000, 'low': 271500, 'close': 276000, 'volume': 558083, 'target_date': datetime.datetime(2000, 1, 27, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 26, 0, 0), 'open': 275000, 'high': 276000, 'low': 270500, 'close': 274000, 'volume': 585734, 'target_date': datetime.datetime(2000, 1, 26, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 25, 0, 0), 'open': 276000, 'high': 282000, 'low': 272000, 'close': 272000, 'volume': 885067, 'target_date': datetime.datetime(2000, 1, 25, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 24, 0, 0), 'open': 290000, 'high': 295000, 'low': 285000, 'close': 285000, 'volume': 687183, 'target_date': datetime.datetime(2000, 1, 24, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 21, 0, 0), 'open': 297500, 'high': 299000, 'low': 294000, 'close': 294000, 'volume': 650728, 'target_date': datetime.datetime(2000, 1, 21, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 20, 0, 0), 'open': 293000, 'high': 302000, 'low': 291000, 'close': 302000, 'volume': 746360, 'target_date': datetime.datetime(2000, 1, 20, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 19, 0, 0), 'open': 300000, 'high': 302000, 'low': 298000, 'close': 298000, 'volume': 804159, 'target_date': datetime.datetime(2000, 1, 19, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 18, 0, 0), 'open': 308000, 'high': 308000, 'low': 299000, 'close': 305000, 'volume': 905231, 'target_date': datetime.datetime(2000, 1, 18, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 17, 0, 0), 'open': 300000, 'high': 309000, 'low': 296000, 'close': 305000, 'volume': 1270138, 'target_date': datetime.datetime(2000, 1, 17, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 14, 0, 0), 'open': 286000, 'high': 294000, 'low': 284000, 'close': 291500, 'volume': 987576, 'target_date': datetime.datetime(2000, 1, 14, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 13, 0, 0), 'open': 280000, 'high': 287000, 'low': 278000, 'close': 285500, 'volume': 823830, 'target_date': datetime.datetime(2000, 1, 13, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 12, 0, 0), 'open': 280500, 'high': 287000, 'low': 280000, 'close': 286000, 'volume': 584492, 'target_date': datetime.datetime(2000, 1, 12, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 11, 0, 0), 'open': 291000, 'high': 305000, 'low': 288500, 'close': 288500, 'volume': 1194974, 'target_date': datetime.datetime(2000, 1, 11, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 10, 0, 0), 'open': 280000, 'high': 288500, 'low': 279000, 'close': 288500, 'volume': 937615, 'target_date': datetime.datetime(2000, 1, 10, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 7, 0, 0), 'open': 278000, 'high': 283500, 'low': 268000, 'close': 277000, 'volume': 806195, 'target_date': datetime.datetime(2000, 1, 7, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 6, 0, 0), 'open': 287500, 'high': 289000, 'low': 279000, 'close': 281000, 'volume': 1087810, 'target_date': datetime.datetime(2000, 1, 6, 0, 0)}, {'code': '005930', 'date': datetime.datetime(2000, 1, 5, 0, 0), 'open': 290000, 'high': 303000, 'low': 276000, 'close': 279000, 'volume': 1493604, 'target_date': datetime.datetime(2000, 1, 5, 0, 0)}]\n",
      "2000-01-05 00:00:00 ~ 2000-01-29 00:00:00 기간의 \n",
      "데이터가 크롤 및 UPDATE 되었습니다. UPDATE COUNT: 18 \n",
      "update_only\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_history_of('삼성전자', \"2000-1-5\", \"2000-1-29\", hard_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YEAR_CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = datetime.datetime.strptime(\"2018-2-1\", \"%Y-%m-%d\")\n",
    "e = datetime.datetime.strptime(\"2018-2-26\", \"%Y-%m-%d\")\n",
    "\n",
    "old = datetime.datetime.strptime(\"2018-2-10\", \"%Y-%m-%d\")\n",
    "latest = datetime.datetime.strptime(\"2018-2-15\", \"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-02-01'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datetime.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s) == datetime.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_time_interval= set(map(pd.Timestamp.to_pydatetime, pd.date_range(old, latest).tolist()))\n",
    "request_time_interval= set(map(pd.Timestamp.to_pydatetime, pd.date_range(s, e).tolist()))\n",
    "\n",
    "# db 데이터의 interval과 원하는 interval의 교집합을 구한다.\n",
    "intersection = db_time_interval & request_time_interval\n",
    "intersection_as_list = list(intersection)\n",
    "intersection_as_list.sort()\n",
    "intersection_start_date = intersection_as_list[0]\n",
    "intersection_end_date = intersection_as_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 2, 10, 0, 0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 2, 15, 0, 0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unique 기간에대해서는 INSERT\n",
    "unique_req = request_time_interval - intersection\n",
    "unique_db = db_time_interval - intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_req_as_list = list(unique_req)\n",
    "unique_req_as_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2018, 2, 1, 0, 0),\n",
       " datetime.datetime(2018, 2, 2, 0, 0),\n",
       " datetime.datetime(2018, 2, 3, 0, 0),\n",
       " datetime.datetime(2018, 2, 4, 0, 0),\n",
       " datetime.datetime(2018, 2, 5, 0, 0),\n",
       " datetime.datetime(2018, 2, 6, 0, 0),\n",
       " datetime.datetime(2018, 2, 7, 0, 0),\n",
       " datetime.datetime(2018, 2, 8, 0, 0),\n",
       " datetime.datetime(2018, 2, 9, 0, 0)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ date for date in unique_req_as_list if date < intersection_start_date ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2018, 2, 1, 0, 0),\n",
       " datetime.datetime(2018, 2, 2, 0, 0),\n",
       " datetime.datetime(2018, 2, 3, 0, 0),\n",
       " datetime.datetime(2018, 2, 4, 0, 0),\n",
       " datetime.datetime(2018, 2, 5, 0, 0),\n",
       " datetime.datetime(2018, 2, 6, 0, 0),\n",
       " datetime.datetime(2018, 2, 7, 0, 0),\n",
       " datetime.datetime(2018, 2, 8, 0, 0),\n",
       " datetime.datetime(2018, 2, 9, 0, 0),\n",
       " datetime.datetime(2018, 2, 16, 0, 0),\n",
       " datetime.datetime(2018, 2, 17, 0, 0),\n",
       " datetime.datetime(2018, 2, 18, 0, 0),\n",
       " datetime.datetime(2018, 2, 19, 0, 0),\n",
       " datetime.datetime(2018, 2, 20, 0, 0),\n",
       " datetime.datetime(2018, 2, 21, 0, 0),\n",
       " datetime.datetime(2018, 2, 22, 0, 0),\n",
       " datetime.datetime(2018, 2, 23, 0, 0),\n",
       " datetime.datetime(2018, 2, 24, 0, 0),\n",
       " datetime.datetime(2018, 2, 25, 0, 0),\n",
       " datetime.datetime(2018, 2, 26, 0, 0)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ date for date in unique_req_as_list if date < intersection_start_date ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_target_item(dic, col_name):\n",
    "    dic['target_'+col_name] = dic.get(col_name)\n",
    "    return dic\n",
    "\n",
    "def update_multiple(table_name, column, data_dict_list):\n",
    "    \"\"\"\n",
    "    하나의 column에 대해서 조건을 줘서\n",
    "    DB에 기존에 있던 데이터에 대해서 한 번에 Update 한다.\n",
    "    \"\"\"\n",
    "    transaction = db._BaseDBAccessLayer__connection.begin() #self.__connection.begin()\n",
    "\n",
    "    if(data_dict_list):\n",
    "        try:\n",
    "            table = db.find_table(table_name)\n",
    "            if(table.c.has_key(column)):\n",
    "                c = table.c.get(column)\n",
    "                \n",
    "                upd = table.update().\\\n",
    "                      where(c == bindparam('target_'+column))\n",
    "                    \n",
    "                added_data_dict_list = [add_target_item(dic, column) for dic in data_dict_list]\n",
    "                result = db._BaseDBAccessLayer__connection.execute(upd, added_data_dict_list)\n",
    "                transaction.commit()\n",
    "                return True\n",
    "                      \n",
    "            else:\n",
    "                print(\"{} table 에는 {} Column 이 없습니다\".format(table_name, column))\n",
    "                print(\"Let's RollBack\")\n",
    "                transaction.rollback()\n",
    "                return False\n",
    "\n",
    "        except IntegrityError as error:\n",
    "            error_code, error_msg = error.orig.args\n",
    "\n",
    "            print(\"IntegrityError!!\")\n",
    "            print(\">>>error_code: {}\".format(error_code))\n",
    "            print(\">>>error_msg: {}\".format(error_msg))\n",
    "            print(\"Let's RollBack\")\n",
    "            transaction.rollback()\n",
    "\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        print(\"빈 dict입니다\")\n",
    "        transaction.rollback()\n",
    "        print(\"Let's RollBack\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> stmt = users.update().\\\n",
    "...             where(users.c.name == bindparam('oldname')).\\\n",
    "...             values(name=bindparam('newname'))\n",
    ">>> conn.execute(stmt, [\n",
    "...     {'oldname':'jack', 'newname':'ed'},\n",
    "...     {'oldname':'wendy', 'newname':'mary'},\n",
    "...     {'oldname':'jim', 'newname':'jake'},\n",
    "...     ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UPDATE users SET name=? WHERE users.name = ?\n",
    "(('ed', 'jack'), ('mary', 'wendy'), ('jake', 'jim'))\n",
    "COMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = db.find_table('hist_data')\n",
    "c = table.c.get('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import and_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upd = table.update().\\\n",
    "      where(c == bindparam('target_date')).\\\n",
    "      values(date= bindparam('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = datetime.datetime.strptime(\"2018-2-20\", \"%Y-%m-%d\")\n",
    "t2 = datetime.datetime.strptime(\"2018-2-21\", \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = [{'target_date':t, 'date':t, 'open':5, 'high':5, 'low':5, 'close':5, 'volume':5 },\n",
    "     {'target_date':t2, 'date':t2, 'open':5, 'high':5, 'low':5, 'close':5, 'volume':5 }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = [{'date':t, 'open':3, 'high':2, 'low':3, 'close':4, 'volume':5 },\n",
    "     {'date':t2, 'open':3, 'high':2, 'low':3, 'close':4, 'volume':5 }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.exc import IntegrityError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_multiple('hist_data', 'date', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'close': 4,\n",
       "  'date': datetime.datetime(2018, 2, 22, 0, 0),\n",
       "  'high': 2,\n",
       "  'low': 3,\n",
       "  'open': 3,\n",
       "  'target_date': datetime.datetime(2018, 2, 22, 0, 0),\n",
       "  'volume': 5},\n",
       " {'close': 4,\n",
       "  'date': datetime.datetime(2018, 2, 23, 0, 0),\n",
       "  'high': 2,\n",
       "  'low': 3,\n",
       "  'open': 3,\n",
       "  'target_date': datetime.datetime(2018, 2, 23, 0, 0),\n",
       "  'volume': 5}]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = db._BaseDBAccessLayer__connection.execute(upd, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = db._BaseDBAccessLayer__connection.begin()\n",
    "e = b.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-89e6c98d9288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datelist = pd.date_range(pd.datetime.today(), periods=100).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'005930'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.find_stock_code('삼성전자')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page num: 1\n",
      "page num: 2\n",
      "page num: 3\n",
      "page num: 4\n",
      "page num: 5\n",
      "page num: 6\n",
      "page num: 7\n",
      "page num: 8\n",
      "page num: 9\n",
      "page num: 10\n",
      "page num: 11\n",
      "page num: 12\n",
      "page num: 13\n",
      "page num: 14\n",
      "page num: 15\n",
      "page num: 16\n",
      "page num: 17\n",
      "page num: 18\n",
      "page num: 19\n",
      "page num: 20\n",
      "page num: 21\n",
      "page num: 22\n",
      "page num: 23\n",
      "page num: 24\n",
      "page num: 25\n",
      "page num: 26\n",
      "page num: 27\n",
      "page num: 28\n",
      "page num: 29\n",
      "page num: 30\n",
      "page num: 31\n",
      "page num: 32\n",
      "page num: 33\n",
      "page num: 34\n",
      "page num: 35\n",
      "page num: 36\n",
      "page num: 37\n",
      "page num: 38\n",
      "page num: 39\n",
      "page num: 40\n",
      "page num: 41\n",
      "page num: 42\n",
      "page num: 43\n",
      "page num: 44\n",
      "page num: 45\n",
      "page num: 46\n",
      "page num: 47\n",
      "page num: 48\n",
      "page num: 49\n",
      "page num: 50\n",
      "page num: 51\n",
      "page num: 52\n",
      "page num: 53\n",
      "page num: 54\n",
      "page num: 55\n",
      "page num: 56\n",
      "page num: 57\n",
      "page num: 58\n",
      "page num: 59\n",
      "page num: 60\n",
      "page num: 61\n",
      "page num: 62\n",
      "page num: 63\n",
      "page num: 64\n",
      "page num: 65\n",
      "page num: 66\n",
      "page num: 67\n",
      "page num: 68\n",
      "page num: 69\n",
      "page num: 70\n",
      "page num: 71\n",
      "page num: 72\n",
      "page num: 73\n",
      "page num: 74\n",
      "page num: 75\n",
      "page num: 76\n",
      "page num: 77\n",
      "page num: 78\n",
      "page num: 79\n",
      "page num: 80\n",
      "page num: 81\n",
      "page num: 82\n",
      "page num: 83\n",
      "page num: 84\n",
      "page num: 85\n",
      "page num: 86\n",
      "page num: 87\n",
      "page num: 88\n",
      "page num: 89\n",
      "page num: 90\n",
      "page num: 91\n",
      "page num: 92\n",
      "page num: 93\n",
      "page num: 94\n",
      "page num: 95\n",
      "page num: 96\n",
      "page num: 97\n",
      "page num: 98\n",
      "page num: 99\n",
      "page num: 100\n",
      "page num: 101\n",
      "page num: 102\n",
      "page num: 103\n",
      "page num: 104\n",
      "page num: 105\n",
      "page num: 106\n",
      "page num: 107\n",
      "page num: 108\n",
      "page num: 109\n",
      "page num: 110\n",
      "page num: 111\n",
      "page num: 112\n",
      "page num: 113\n",
      "page num: 114\n",
      "page num: 115\n",
      "page num: 116\n",
      "page num: 117\n",
      "page num: 118\n",
      "page num: 119\n",
      "page num: 120\n",
      "page num: 121\n",
      "page num: 122\n",
      "page num: 123\n",
      "page num: 124\n",
      "page num: 125\n",
      "page num: 126\n",
      "page num: 127\n",
      "page num: 128\n",
      "page num: 129\n",
      "page num: 130\n",
      "page num: 131\n",
      "page num: 132\n",
      "page num: 133\n",
      "page num: 134\n",
      "page num: 135\n",
      "page num: 136\n",
      "page num: 137\n",
      "page num: 138\n",
      "page num: 139\n",
      "page num: 140\n",
      "page num: 141\n",
      "page num: 142\n",
      "page num: 143\n",
      "page num: 144\n",
      "page num: 145\n",
      "[[datetime.datetime(2000, 1, 5, 0, 0), 290000, 303000, 276000, 279000, 1493604],\n",
      " [datetime.datetime(2000, 1, 4, 0, 0), 300000, 305500, 283000, 305500, 1483967],\n",
      " [datetime.datetime(1999, 12, 28, 0, 0),\n",
      "  260000,\n",
      "  278000,\n",
      "  252500,\n",
      "  266000,\n",
      "  1481697],\n",
      " [datetime.datetime(1999, 12, 27, 0, 0),\n",
      "  273000,\n",
      "  274500,\n",
      "  257000,\n",
      "  261000,\n",
      "  934057],\n",
      " [datetime.datetime(1999, 12, 24, 0, 0),\n",
      "  285000,\n",
      "  286000,\n",
      "  268000,\n",
      "  273000,\n",
      "  708446]]\n",
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "crawl_from_a_to_b('삼성전자', \"1999-12-24\", \"2000-1-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req = requests.get(f'http://finance.daum.net/item/quote_yyyymmdd_sub.daum?page=005930&code=090430&modify=1')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "d = len(soup.find_all('td', 'datetime2'))*5\n",
    "p = len(soup.find_all(filter_for_history, string=is_the_only_string_within_a_tag))\n",
    "        \n",
    "price = list(chunks([text_to_num(tag) for tag in soup.find_all(filter_for_history, string= is_the_only_string_within_a_tag)], 5))\n",
    "date = string_to_datetime(soup.find_all('td','datetime2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crawl_historical_data_of(company_name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    page = 93 \n",
    "    code = db.find_stock_code(company_name)\n",
    "    \n",
    "    while True:\n",
    "        req = requests.get(f'http://finance.daum.net/item/quote_yyyymmdd_sub.daum?page={page}&code=090430&modify=1')\n",
    "        html = req.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        d = len(soup.find_all('td', 'datetime2'))*5\n",
    "        p = len(soup.find_all(filter_for_history, string=is_the_only_string_within_a_tag))\n",
    "\n",
    "        \n",
    "        price = list(chunks([text_to_num(tag) for tag in soup.find_all(filter_for_history, string= is_the_only_string_within_a_tag)], 5))\n",
    "        date = string_to_datetime(soup.find_all('td','datetime2'))\n",
    "\n",
    "        if(p == 150 and d==150):\n",
    "            page +=1\n",
    "            #pprint.pprint([[d] + p for d, p in zip(date, price)])\n",
    "            yield [[code, d] + p for d, p in zip(date, price)]\n",
    "        else:\n",
    "            #pprint.pprint([[d] + p for d, p in zip(date, price)])\n",
    "            yield [[code, d] + p for d, p in zip(date, price)]\n",
    "            print(\"<--------------------------------------------------------->\")\n",
    "            print(f\"page: {page}\")\n",
    "            print(f\"datetime*5: {d}\")\n",
    "            print(f\"price: {p}\")\n",
    "            print(f\"{d==p}\")\n",
    "            YEAR_CHANGED = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'close': 40410,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 21, 0, 0),\n",
      "  'high': 40660,\n",
      "  'low': 40110,\n",
      "  'open': 40510,\n",
      "  'volume': 164778},\n",
      " {'close': 40560,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 18, 0, 0),\n",
      "  'high': 41510,\n",
      "  'low': 40560,\n",
      "  'open': 40910,\n",
      "  'volume': 113121},\n",
      " {'close': 41460,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 17, 0, 0),\n",
      "  'high': 41560,\n",
      "  'low': 40510,\n",
      "  'open': 41110,\n",
      "  'volume': 141264},\n",
      " {'close': 41560,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 16, 0, 0),\n",
      "  'high': 41860,\n",
      "  'low': 41560,\n",
      "  'open': 41810,\n",
      "  'volume': 65833},\n",
      " {'close': 41810,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 14, 0, 0),\n",
      "  'high': 41910,\n",
      "  'low': 41410,\n",
      "  'open': 41910,\n",
      "  'volume': 46078},\n",
      " {'close': 41860,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 11, 0, 0),\n",
      "  'high': 42010,\n",
      "  'low': 41210,\n",
      "  'open': 41910,\n",
      "  'volume': 45428},\n",
      " {'close': 41910,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 10, 0, 0),\n",
      "  'high': 41910,\n",
      "  'low': 41260,\n",
      "  'open': 41560,\n",
      "  'volume': 50317},\n",
      " {'close': 41860,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 9, 0, 0),\n",
      "  'high': 41860,\n",
      "  'low': 40510,\n",
      "  'open': 40510,\n",
      "  'volume': 70442},\n",
      " {'close': 41510,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 8, 0, 0),\n",
      "  'high': 41510,\n",
      "  'low': 40560,\n",
      "  'open': 40560,\n",
      "  'volume': 71812},\n",
      " {'close': 40960,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 7, 0, 0),\n",
      "  'high': 41210,\n",
      "  'low': 40710,\n",
      "  'open': 41010,\n",
      "  'volume': 43419},\n",
      " {'close': 41110,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 4, 0, 0),\n",
      "  'high': 41110,\n",
      "  'low': 40360,\n",
      "  'open': 40460,\n",
      "  'volume': 164428},\n",
      " {'close': 40460,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 3, 0, 0),\n",
      "  'high': 40810,\n",
      "  'low': 39510,\n",
      "  'open': 40810,\n",
      "  'volume': 68012},\n",
      " {'close': 40210,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 2, 0, 0),\n",
      "  'high': 40310,\n",
      "  'low': 39910,\n",
      "  'open': 40210,\n",
      "  'volume': 67803},\n",
      " {'close': 40110,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 8, 1, 0, 0),\n",
      "  'high': 40110,\n",
      "  'low': 39810,\n",
      "  'open': 40110,\n",
      "  'volume': 70752},\n",
      " {'close': 40110,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 31, 0, 0),\n",
      "  'high': 40260,\n",
      "  'low': 39810,\n",
      "  'open': 40060,\n",
      "  'volume': 99294},\n",
      " {'close': 40010,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 28, 0, 0),\n",
      "  'high': 40010,\n",
      "  'low': 39460,\n",
      "  'open': 39960,\n",
      "  'volume': 107402},\n",
      " {'close': 39960,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 27, 0, 0),\n",
      "  'high': 40310,\n",
      "  'low': 39810,\n",
      "  'open': 39960,\n",
      "  'volume': 204497},\n",
      " {'close': 39510,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 26, 0, 0),\n",
      "  'high': 40310,\n",
      "  'low': 39510,\n",
      "  'open': 40010,\n",
      "  'volume': 396268},\n",
      " {'close': 40010,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 25, 0, 0),\n",
      "  'high': 41460,\n",
      "  'low': 39810,\n",
      "  'open': 41460,\n",
      "  'volume': 174545},\n",
      " {'close': 40060,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 24, 0, 0),\n",
      "  'high': 40060,\n",
      "  'low': 39410,\n",
      "  'open': 39410,\n",
      "  'volume': 232910},\n",
      " {'close': 39410,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 21, 0, 0),\n",
      "  'high': 40710,\n",
      "  'low': 39360,\n",
      "  'open': 40210,\n",
      "  'volume': 111801},\n",
      " {'close': 40810,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 20, 0, 0),\n",
      "  'high': 40910,\n",
      "  'low': 40010,\n",
      "  'open': 40760,\n",
      "  'volume': 123578},\n",
      " {'close': 40510,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 19, 0, 0),\n",
      "  'high': 41010,\n",
      "  'low': 40160,\n",
      "  'open': 40160,\n",
      "  'volume': 119159},\n",
      " {'close': 40510,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 18, 0, 0),\n",
      "  'high': 43011,\n",
      "  'low': 40510,\n",
      "  'open': 43011,\n",
      "  'volume': 208626},\n",
      " {'close': 43061,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 14, 0, 0),\n",
      "  'high': 45862,\n",
      "  'low': 41661,\n",
      "  'open': 42711,\n",
      "  'volume': 99604},\n",
      " {'close': 43011,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 13, 0, 0),\n",
      "  'high': 44411,\n",
      "  'low': 42161,\n",
      "  'open': 43811,\n",
      "  'volume': 181733},\n",
      " {'close': 44511,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 12, 0, 0),\n",
      "  'high': 45111,\n",
      "  'low': 43261,\n",
      "  'open': 44811,\n",
      "  'volume': 199009},\n",
      " {'close': 45511,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 11, 0, 0),\n",
      "  'high': 45711,\n",
      "  'low': 44811,\n",
      "  'open': 45361,\n",
      "  'volume': 202218},\n",
      " {'close': 45761,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 10, 0, 0),\n",
      "  'high': 45861,\n",
      "  'low': 44611,\n",
      "  'open': 44611,\n",
      "  'volume': 139544},\n",
      " {'close': 45811,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2006, 7, 7, 0, 0),\n",
      "  'high': 45911,\n",
      "  'low': 44411,\n",
      "  'open': 45061,\n",
      "  'volume': 294374}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint([listdata_to_dict(oneday_info) for oneday_info in sample[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([listdata_to_dict(oneday_info) for oneday_info in sample[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[datetime.datetime(2018, 2, 26, 0, 0),\n",
      "  2364000,\n",
      "  2378000,\n",
      "  2354000,\n",
      "  2358000,\n",
      "  70658],\n",
      " [datetime.datetime(2018, 2, 23, 0, 0),\n",
      "  2338000,\n",
      "  2390000,\n",
      "  2338000,\n",
      "  2361000,\n",
      "  248466],\n",
      " [datetime.datetime(2018, 2, 22, 0, 0),\n",
      "  2363000,\n",
      "  2363000,\n",
      "  2338000,\n",
      "  2338000,\n",
      "  177399],\n",
      " [datetime.datetime(2018, 2, 21, 0, 0),\n",
      "  2364000,\n",
      "  2379000,\n",
      "  2342000,\n",
      "  2364000,\n",
      "  257604],\n",
      " [datetime.datetime(2018, 2, 20, 0, 0),\n",
      "  2402000,\n",
      "  2408000,\n",
      "  2361000,\n",
      "  2370000,\n",
      "  202452]]\n",
      "총 5의 데이터\n",
      "5\n",
      "[{'close': 2358000,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2018, 2, 26, 0, 0),\n",
      "  'high': 2378000,\n",
      "  'low': 2354000,\n",
      "  'open': 2364000,\n",
      "  'volume': 70658},\n",
      " {'close': 2361000,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2018, 2, 23, 0, 0),\n",
      "  'high': 2390000,\n",
      "  'low': 2338000,\n",
      "  'open': 2338000,\n",
      "  'volume': 248466},\n",
      " {'close': 2338000,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2018, 2, 22, 0, 0),\n",
      "  'high': 2363000,\n",
      "  'low': 2338000,\n",
      "  'open': 2363000,\n",
      "  'volume': 177399},\n",
      " {'close': 2364000,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2018, 2, 21, 0, 0),\n",
      "  'high': 2379000,\n",
      "  'low': 2342000,\n",
      "  'open': 2364000,\n",
      "  'volume': 257604},\n",
      " {'close': 2370000,\n",
      "  'code': '005930',\n",
      "  'date': datetime.datetime(2018, 2, 20, 0, 0),\n",
      "  'high': 2408000,\n",
      "  'low': 2361000,\n",
      "  'open': 2402000,\n",
      "  'volume': 202452}]\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "data_count = 0\n",
    "historical_data_list = []\n",
    "\n",
    "data_bomb = base_crawl_method_for_history('삼성전자', \"2018-2-20\", \"2018-2-27\")\n",
    "\n",
    "for data in data_bomb:\n",
    "    data_count += len(data)\n",
    "    #pprint.pprint(listdata_to_dict(data))\n",
    "    historical_data_list += [listdata_to_dict(oneday_info) for oneday_info in data]\n",
    "\n",
    "    if(data_count == 100):\n",
    "        total_count +=100\n",
    "        # 종목코드 DB에 저장한다.\n",
    "        data_count = 0\n",
    "        is_okay = db.insert_multiple('hist_data', historical_data_list)\n",
    "        historical_data_list = []\n",
    "\n",
    "is_okay = db.insert_multiple('hist_data', historical_data_list)\n",
    "total_count += data_count\n",
    "print(\"총 {}의 데이터\".format(total_count))\n",
    "print(len(historical_data_list))\n",
    "pprint.pprint(historical_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['005930',\n",
      "  datetime.datetime(2018, 2, 26, 0, 0),\n",
      "  2364000,\n",
      "  2378000,\n",
      "  2354000,\n",
      "  2369000,\n",
      "  163980],\n",
      " ['005930',\n",
      "  datetime.datetime(2018, 2, 23, 0, 0),\n",
      "  2338000,\n",
      "  2390000,\n",
      "  2338000,\n",
      "  2361000,\n",
      "  248466],\n",
      " ['005930',\n",
      "  datetime.datetime(2018, 2, 22, 0, 0),\n",
      "  2363000,\n",
      "  2363000,\n",
      "  2338000,\n",
      "  2338000,\n",
      "  177399],\n",
      " ['005930',\n",
      "  datetime.datetime(2018, 2, 21, 0, 0),\n",
      "  2364000,\n",
      "  2379000,\n",
      "  2342000,\n",
      "  2364000,\n",
      "  257604],\n",
      " ['005930',\n",
      "  datetime.datetime(2018, 2, 20, 0, 0),\n",
      "  2402000,\n",
      "  2408000,\n",
      "  2361000,\n",
      "  2370000,\n",
      "  202452]]\n",
      "<><><><><><><><><><><><><><>\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "data_bomb = base_crawl_method_for_history('삼성전자', \"2018-2-20\", \"2018-2-27\")\n",
    "\n",
    "for data in data_bomb:\n",
    "    print(\"<><><><><><><><><><><><><><>\")\n",
    "    temp += [listdata_to_dict(oneday_info) for oneday_info in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'close': 2369000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 26, 0, 0),\n",
       "  'high': 2378000,\n",
       "  'low': 2354000,\n",
       "  'open': 2364000,\n",
       "  'volume': 163980},\n",
       " {'close': 2361000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 23, 0, 0),\n",
       "  'high': 2390000,\n",
       "  'low': 2338000,\n",
       "  'open': 2338000,\n",
       "  'volume': 248466},\n",
       " {'close': 2338000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 22, 0, 0),\n",
       "  'high': 2363000,\n",
       "  'low': 2338000,\n",
       "  'open': 2363000,\n",
       "  'volume': 177399},\n",
       " {'close': 2364000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 21, 0, 0),\n",
       "  'high': 2379000,\n",
       "  'low': 2342000,\n",
       "  'open': 2364000,\n",
       "  'volume': 257604},\n",
       " {'close': 2370000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 20, 0, 0),\n",
       "  'high': 2408000,\n",
       "  'low': 2361000,\n",
       "  'open': 2402000,\n",
       "  'volume': 202452}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "added = [add_target_item(dic, 'date') for dic in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'close': 2369000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 26, 0, 0),\n",
       "  'high': 2378000,\n",
       "  'low': 2354000,\n",
       "  'open': 2364000,\n",
       "  'target_date': datetime.datetime(2018, 2, 26, 0, 0),\n",
       "  'volume': 163980},\n",
       " {'close': 2361000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 23, 0, 0),\n",
       "  'high': 2390000,\n",
       "  'low': 2338000,\n",
       "  'open': 2338000,\n",
       "  'target_date': datetime.datetime(2018, 2, 23, 0, 0),\n",
       "  'volume': 248466},\n",
       " {'close': 2338000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 22, 0, 0),\n",
       "  'high': 2363000,\n",
       "  'low': 2338000,\n",
       "  'open': 2363000,\n",
       "  'target_date': datetime.datetime(2018, 2, 22, 0, 0),\n",
       "  'volume': 177399},\n",
       " {'close': 2364000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 21, 0, 0),\n",
       "  'high': 2379000,\n",
       "  'low': 2342000,\n",
       "  'open': 2364000,\n",
       "  'target_date': datetime.datetime(2018, 2, 21, 0, 0),\n",
       "  'volume': 257604},\n",
       " {'close': 2370000,\n",
       "  'code': '005930',\n",
       "  'date': datetime.datetime(2018, 2, 20, 0, 0),\n",
       "  'high': 2408000,\n",
       "  'low': 2361000,\n",
       "  'open': 2402000,\n",
       "  'target_date': datetime.datetime(2018, 2, 20, 0, 0),\n",
       "  'volume': 202452}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is empty dict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.insert_multiple('hist_data', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_dict = {\n",
    "                'code': data[0],        # 종목코드\n",
    "                'date': data[1],        # 날짜\n",
    "                'price_close': int(data[2].replace(',', '')), # 종가\n",
    "                'price_open': int(data[3].replace(',', '')),  # 시가\n",
    "                'price_high': int(data[4].replace(',', '')),  # 고가\n",
    "                'price_low' : int(data[5].replace(',', '')),  # 저가\n",
    "                'volume'    : int(data[6].replace(',', '')),  # 거래량\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[datetime.datetime(2006, 8, 18, 0, 0), 40910, 41510, 40560, 40560, 113121],\n",
      " [datetime.datetime(2006, 8, 17, 0, 0), 41110, 41560, 40510, 41460, 141264],\n",
      " [datetime.datetime(2006, 8, 16, 0, 0), 41810, 41860, 41560, 41560, 65833],\n",
      " [datetime.datetime(2006, 8, 14, 0, 0), 41910, 41910, 41410, 41810, 46078],\n",
      " [datetime.datetime(2006, 8, 11, 0, 0), 41910, 42010, 41210, 41860, 45428],\n",
      " [datetime.datetime(2006, 8, 10, 0, 0), 41560, 41910, 41260, 41910, 50317],\n",
      " [datetime.datetime(2006, 8, 9, 0, 0), 40510, 41860, 40510, 41860, 70442],\n",
      " [datetime.datetime(2006, 8, 8, 0, 0), 40560, 41510, 40560, 41510, 71812],\n",
      " [datetime.datetime(2006, 8, 7, 0, 0), 41010, 41210, 40710, 40960, 43419],\n",
      " [datetime.datetime(2006, 8, 4, 0, 0), 40460, 41110, 40360, 41110, 164428],\n",
      " [datetime.datetime(2006, 8, 3, 0, 0), 40810, 40810, 39510, 40460, 68012],\n",
      " [datetime.datetime(2006, 8, 2, 0, 0), 40210, 40310, 39910, 40210, 67803],\n",
      " [datetime.datetime(2006, 8, 1, 0, 0), 40110, 40110, 39810, 40110, 70752],\n",
      " [datetime.datetime(2006, 7, 31, 0, 0), 40060, 40260, 39810, 40110, 99294],\n",
      " [datetime.datetime(2006, 7, 28, 0, 0), 39960, 40010, 39460, 40010, 107402],\n",
      " [datetime.datetime(2006, 7, 27, 0, 0), 39960, 40310, 39810, 39960, 204497],\n",
      " [datetime.datetime(2006, 7, 26, 0, 0), 40010, 40310, 39510, 39510, 396268],\n",
      " [datetime.datetime(2006, 7, 25, 0, 0), 41460, 41460, 39810, 40010, 174545],\n",
      " [datetime.datetime(2006, 7, 24, 0, 0), 39410, 40060, 39410, 40060, 232910],\n",
      " [datetime.datetime(2006, 7, 21, 0, 0), 40210, 40710, 39360, 39410, 111801],\n",
      " [datetime.datetime(2006, 7, 20, 0, 0), 40760, 40910, 40010, 40810, 123578],\n",
      " [datetime.datetime(2006, 7, 19, 0, 0), 40160, 41010, 40160, 40510, 119159],\n",
      " [datetime.datetime(2006, 7, 18, 0, 0), 43011, 43011, 40510, 40510, 208626],\n",
      " [datetime.datetime(2006, 7, 14, 0, 0), 42711, 45862, 41661, 43061, 99604],\n",
      " [datetime.datetime(2006, 7, 13, 0, 0), 43811, 44411, 42161, 43011, 181733],\n",
      " [datetime.datetime(2006, 7, 12, 0, 0), 44811, 45111, 43261, 44511, 199009],\n",
      " [datetime.datetime(2006, 7, 11, 0, 0), 45361, 45711, 44811, 45511, 202218],\n",
      " [datetime.datetime(2006, 7, 10, 0, 0), 44611, 45861, 44611, 45761, 139544],\n",
      " [datetime.datetime(2006, 7, 7, 0, 0), 45061, 45911, 44411, 45811, 294374],\n",
      " [datetime.datetime(2006, 7, 6, 0, 0), 44011, 44811, 43661, 44411, 227741]]\n",
      "[[datetime.datetime(2006, 7, 5, 0, 0), 42711, 44011, 42711, 44011, 182793],\n",
      " [datetime.datetime(2006, 7, 4, 0, 0), 41560, 44311, 41560, 44011, 277299],\n",
      " [datetime.datetime(2006, 7, 3, 0, 0), 41310, 41710, 39910, 41460, 207367],\n",
      " [datetime.datetime(2006, 6, 30, 0, 0), 39610, 40960, 39010, 40960, 689333],\n",
      " [datetime.datetime(2006, 6, 29, 0, 0), 38009, 40509, 37509, 38509, 1195342]]\n",
      "<--------------------------------------------------------->\n",
      "page: 97\n",
      "datetime*5: 25\n",
      "price: 25\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "page = 96\n",
    "while True:\n",
    "    req = requests.get(f'http://finance.daum.net/item/quote_yyyymmdd_sub.daum?page={page}&code=090430&modify=1')\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    d = len(soup.find_all('td', 'datetime2'))*5\n",
    "    p = len(soup.find_all(filter_for_history, string=is_the_only_string_within_a_tag))\n",
    "    \n",
    "    \n",
    "    price = list(chunks([text_to_num(tag) for tag in soup.find_all(filter_for_history, string= is_the_only_string_within_a_tag)], 5))\n",
    "    date = string_to_datetime(soup.find_all('td','datetime2'))\n",
    "    \n",
    "    if(p == 150 and d==150):\n",
    "        page +=1\n",
    "        pprint.pprint([[d] + p for d, p in zip(date, price)])\n",
    "    else:\n",
    "        pprint.pprint([[d] + p for d, p in zip(date, price)])\n",
    "        print(\"<--------------------------------------------------------->\")\n",
    "        print(f\"page: {page}\")\n",
    "        print(f\"datetime*5: {d}\")\n",
    "        print(f\"price: {p}\")\n",
    "        print(f\"{d==p}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawler = BigBull.StockCodeCrawler(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawler.save_stock_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
